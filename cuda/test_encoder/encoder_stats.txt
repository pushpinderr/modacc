layer_norm:0.017835
qkv_linear:0.017191
launch_bias_add_transform_0213:0.003151
attention_scores:0.006396
softmax:0.129545
attention_probability_dropout:0.006309
attention_context:0.003001
launch_transform4d_0213:0.000004
attention_out_linear:0.007061
attention_output_dropout:0.002686
attention_layer_norm:0.010708
1st_feed_forward_layer:0.009686
gelu:0.010498
2nd_feed_forward_layer:0.050368
layer_output_dropout:0.002683
